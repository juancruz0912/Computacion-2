# üåê Sistema de Scraping y An√°lisis Web Distribuido

Sistema distribuido de scraping web con procesamiento paralelo, rate limiting y an√°lisis avanzados.

## üìã Tabla de Contenidos

- [Descripci√≥n](#-descripci√≥n)
- [Arquitectura](#-Arquitectura)
- [Caracter√≠sticas](#-caracter√≠sticas)
- [Requisitos](#-requisitos)
- [Instalaci√≥n](#-instalaci√≥n)
- [Uso](#-uso)
- [API Reference](#-api-reference)
- [Bonus Tracks Implementados](#-bonus-tracks-implementados)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Monitoreo](#-monitoreo)
- [Troubleshooting](#-troubleshooting)
- [Notas](#-notas)
- [Autor](#-autor)

---

## üéØ Descripci√≥n

Sistema de scraping web distribuido que separa las responsabilidades de extracci√≥n (Servidor A) y procesamiento intensivo (Servidor B), utilizando comunicaci√≥n TCP as√≠ncrona y procesamiento paralelo con multiprocessing.

### Servidores

- **Servidor A (Scraping)**: Servidor HTTP as√≠ncrono que realiza scraping b√°sico de p√°ginas web
- **Servidor B (Processing)**: Servidor TCP que procesa tareas pesadas usando pool de procesos

---

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CLIENTE                              ‚îÇ
‚îÇ                    (curl, browser, etc)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ HTTP
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SERVIDOR A (Scraping)                     ‚îÇ
‚îÇ                   Port 8000 (HTTP/aiohttp)                   ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Recibe requests HTTP                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Rate Limiting (Redis)                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Cach√© (Redis)                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Extrae HTML, links, im√°genes, metadatos          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Delega procesamiento pesado al Servidor B        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ TCP (Protocol binario)
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  SERVIDOR B (Processing)                     ‚îÇ
‚îÇ                  Port 9000 (TCP/socketserver)                ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Pool de Procesos (multiprocessing.Pool)            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Screenshot ‚îÇ  ‚îÇPerformance ‚îÇ  ‚îÇ  Images    ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (Selenium) ‚îÇ  ‚îÇ (Selenium) ‚îÇ  ‚îÇ (PIL/CV2)  ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇTechnologies ‚îÇ  ‚îÇ   SEO    ‚îÇ                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Detector   ‚îÇ  ‚îÇ Analyzer ‚îÇ                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ    Redis    ‚îÇ
                  ‚îÇ   Port 6379 ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚ú® Caracter√≠sticas

### Scraping (Servidor A)
- ‚úÖ Extracci√≥n de t√≠tulo, texto y estructura HTML
- ‚úÖ Detecci√≥n de links e im√°genes
- ‚úÖ Extracci√≥n de metadatos (SEO, Open Graph, Twitter Cards)
- ‚úÖ Rate Limiting por dominio usando Redis
- ‚úÖ Sistema de cach√© con TTL configurable
- ‚úÖ Comunicaci√≥n as√≠ncrona con aiohttp

### Procesamiento (Servidor B)
- ‚úÖ Screenshots con Selenium WebDriver
- ‚úÖ An√°lisis de rendimiento (load time, recursos)
- ‚úÖ Procesamiento de im√°genes (descarga, thumbnails, dimensiones)
- ‚úÖ Detecci√≥n de tecnolog√≠as web (frameworks, CMS, librer√≠as, analytics)
- ‚úÖ An√°lisis completo de SEO con scoring
- ‚úÖ Pool de procesos para paralelizaci√≥n
- ‚úÖ Soporte para IPv4/IPv6

### Sistema
- ‚úÖ Protocolo binario personalizado con JSON
- ‚úÖ Manejo robusto de errores
- ‚úÖ Logging detallado
- ‚úÖ Tests automatizados

---

## üì¶ Requisitos

### Software Base
- Python 3.8+
- Redis Server 7.0+
- Chrome/Chromium (para Selenium)
- ChromeDriver

### Dependencias Python
```bash
pip install -r requirements.txt
```

**requirements.txt:**
```
aiohttp>=3.9.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
Pillow>=10.0.0
selenium>=4.15.0
redis>=7.0.0
```

---

## üöÄ Instalaci√≥n

### 1. Clonar repositorio
```bash
git clone <repo>
cd TP2
```

### 2. Crear entorno virtual
```bash
python3 -m venv env
source env/bin/activate  # Linux/Mac
# env\Scripts\activate   # Windows
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 4. Instalar Redis
```bash
# Ubuntu/Debian
sudo apt-get install redis-server

# macOS
brew install redis

# Iniciar Redis
redis-server
```

### 5. Instalar Chrome y ChromeDriver
```bash
# Ubuntu/Debian
sudo apt-get install chromium-browser chromium-chromedriver

# macOS
brew install --cask google-chrome
brew install chromedriver
```

---

## üíª Uso

### Iniciar Servidores

#### Terminal 1: Redis (si no est√° en systemd)
```bash
redis-server
```

#### Terminal 2: Servidor B (Processing)
```bash
python server_processing.py -i localhost -p 9000
```

**Opciones:**
- `-i, --ip`: Direcci√≥n de escucha (default: localhost)
- `-p, --port`: Puerto TCP (default: 9000)
- `-n, --processes`: N√∫mero de procesos en el pool (default: CPU count)

#### Terminal 3: Servidor A (Scraping)
```bash
python server_scraping.py -i localhost -p 8000
```

**Opciones:**
- `-i, --ip`: Direcci√≥n de escucha (default: localhost)
- `-p, --port`: Puerto HTTP (default: 8000)
- `--processing-host`: IP del servidor B (default: localhost)
- `--processing-port`: Puerto del servidor B (default: 9000)
- `--redis-host`: IP de Redis (default: localhost)
- `--redis-port`: Puerto de Redis (default: 6379)
- `--no-cache`: Deshabilitar sistema de cach√©
- `--no-rate-limit`: Deshabilitar rate limiting
- `--max-requests`: M√°ximo requests/min por dominio (default: 10)
- `--cache-ttl`: TTL de cach√© en segundos (default: 3600)

### Ejemplos de Uso

#### 1. Scraping b√°sico
```bash
curl "http://localhost:8000/scrape?url=https://example.com"
```

**Respuesta:**
```json
{
  "url": "https://example.com",
  "status": "success",
  "scraping_data": {
    "basic": {
      "title": "Example Domain",
      "text_preview": "...",
      "word_count": 21
    },
    "structure": {
      "headers": {"h1": 1, "h2": 0, ...},
      "elements_count": {...}
    },
    "links": [...],
    "images": [...],
    "metadata": {...}
  }
}
```

#### 2. Scraping completo (con procesamiento)
```bash
curl "http://localhost:8000/scrape?url=https://example.com&full=true"
```

**Respuesta incluye:**
- `scraping_data`: Datos b√°sicos extra√≠dos
- `processing_data`:
  - `screenshot`: Captura de pantalla
  - `performance`: M√©tricas de rendimiento
  - `images`: Im√°genes procesadas
  - `technologies`: Tecnolog√≠as detectadas (frameworks, CMS, etc.)
  - `seo`: An√°lisis completo de SEO con score

#### 3. Health check
```bash
curl "http://localhost:8000/health"
```

#### 4. Estad√≠sticas de cach√©
```bash
curl "http://localhost:8000/cache/stats"
```

**Respuesta:**
```json
{
  "cache_stats": {
    "hits": 15,
    "misses": 5,
    "writes": 5,
    "total_requests": 20,
    "hit_rate_percent": 75.0
  }
}
```

#### 5. Limpiar cach√©
```bash
curl -X POST "http://localhost:8000/cache/clear"
```

---

## üìö API Reference

### Endpoints del Servidor A

#### `GET /health`
Health check del servidor.


**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-11-13T15:00:00.000000Z",
  "services": {
    "rate_limiter": "enabled",
    "cache": "enabled"
  },
  "cache_stats": {...}
}
```

#### `GET /scrape`
Realiza scraping de una URL.

**Par√°metros:**
- `url` (required): URL a scrapear
- `full` (optional): `true` para procesamiento completo (default: `false`)

**Headers de respuesta:**
- `X-Cache`: `HIT` o `MISS`
- `X-Cache-TTL`: Segundos restantes de TTL (si es HIT)
- `X-RateLimit-Limit`: L√≠mite de requests por ventana
- `X-RateLimit-Remaining`: Requests restantes

**Status codes:**
- `200`: Success
- `400`: Par√°metros inv√°lidos
- `429`: Rate limit excedido
- `500`: Error interno

#### `GET /cache/stats`
Obtiene estad√≠sticas del sistema de cach√©.

#### `POST /cache/clear`
Limpia toda la cach√©.

---

## üéÅ Bonus Tracks Implementados

### ‚úÖ Bonus Track 2: Rate Limiting y Cach√© con Redis

#### Rate Limiter
- **Implementaci√≥n**: Ventana deslizante con Redis Sorted Sets
- **Granularidad**: Por dominio (evita bloquear todo el scraper)
- **Configurable**: L√≠mite de requests y ventana de tiempo ajustables
- **Ubicaci√≥n**: `common/rate_limiter.py`

**Caracter√≠sticas:**
```python
# L√≠mite: 10 requests por minuto por dominio
# Ventana deslizante de 60 segundos
rate_limiter = RateLimiter(
    redis_host='localhost',
    redis_port=6379,
    max_requests=10,
    window_seconds=60
)

# Verificar si se puede procesar
allowed, info = rate_limiter.check_rate_limit(url)
```

**Response cuando se excede:**
```json
{
  "error": "Rate limit exceeded",
  "message": "Too many requests to example.com",
  "rate_limit": {
    "limit": 10,
    "window_seconds": 60,
    "retry_after": 60
  }
}
```

#### Sistema de Cach√©
- **Implementaci√≥n**: Redis con serializaci√≥n JSON
- **TTL**: Configurable por entrada (default: 1 hora)
- **Keys hasheadas**: URLs largas se hashean con SHA-256
- **Cach√© separado**: Diferencia entre scraping b√°sico y completo
- **Ubicaci√≥n**: `common/cache.py`

**Caracter√≠sticas:**
```python
cache = RedisCache(
    redis_host='localhost',
    redis_port=6379,
    default_ttl=3600,  # 1 hora
    key_prefix='scraper'
)

# Guardar en cach√©
cache.set(url, data, full=True, ttl=3600)

# Obtener de cach√©
cached = cache.get(url, full=True)

# Estad√≠sticas
stats = cache.get_stats()
# {
#   'hits': 150,
#   'misses': 50,
#   'writes': 50,
#   'hit_rate_percent': 75.0
# }
```

**Testing:**
```bash
# Test de Rate Limiter
python tests/test_rate_limiter.py

# Test de Cach√©
python tests/test_cache.py
```

---

### ‚úÖ Bonus Track 3: An√°lisis Avanzados

#### 1. Detector de Tecnolog√≠as (`processor/technology_detector.py`)

Detecta autom√°ticamente las tecnolog√≠as utilizadas en una p√°gina web.

**Categor√≠as detectadas:**
- **Frameworks JS**: React, Vue.js, Angular, Next.js, Nuxt.js, Svelte, Ember.js
- **CMS**: WordPress, Drupal, Joomla, Shopify, Wix, Squarespace, Magento, PrestaShop
- **Librer√≠as**: jQuery, Bootstrap, Tailwind CSS, Font Awesome, Lodash, Moment.js, Chart.js, Three.js
- **Analytics**: Google Analytics, Google Tag Manager, Facebook Pixel, Hotjar, Mixpanel, Segment
- **Servidores**: Nginx, Apache, Cloudflare, IIS, LiteSpeed, PHP, ASP.NET, Express.js

**Response:**
```json
{
  "technologies": {
    "frameworks": ["React", "Next.js"],
    "cms": ["WordPress"],
    "libraries": ["jQuery", "Bootstrap"],
    "analytics": ["Google Analytics"],
    "servers": ["Nginx", "PHP"],
    "meta": {
      "generator": "WordPress 6.4"
    },
    "summary": {
      "total_technologies": 6,
      "categories": {
        "frameworks": 2,
        "cms": 1,
        "libraries": 2,
        "analytics": 1,
        "servers": 2
      }
    }
  }
}
```

#### 2. Analizador de SEO (`processor/seo_analyzer.py`)

An√°lisis completo de SEO con scoring de 0-100 y grade A-F.

**Aspectos evaluados:**
- **Title Tag**: Existencia, longitud √≥ptima (50-60 chars)
- **Meta Description**: Existencia, longitud √≥ptima (150-160 chars)
- **Headers**: Jerarqu√≠a H1-H6, un √∫nico H1
- **Im√°genes**: Alt tags presentes, cobertura
- **Links**: Balance interno/externo
- **Open Graph**: Tags para redes sociales
- **Structured Data**: JSON-LD, Schema.org
- **Canonical URL**: Prevenci√≥n de contenido duplicado

**Response:**
```json
{
  "seo": {
    "score": 85,
    "grade": "B",
    "title": {
      "exists": true,
      "text": "Example Domain",
      "length": 14
    },
    "meta_description": {
      "exists": true,
      "text": "This domain is for use in illustrative examples...",
      "length": 145
    },
    "headers": {
      "h1_count": 1,
      "h1_texts": ["Example Domain"],
      "hierarchy": {
        "h1": 1,
        "h2": 3,
        "h3": 5
      }
    },
    "images": {
      "total_images": 10,
      "images_without_alt": 2,
      "alt_coverage_percent": 80.0
    },
    "links": {
      "total_links": 25,
      "internal_links": 18,
      "external_links": 7,
      "ratio": 0.72
    },
    "open_graph": {
      "exists": true,
      "tags": {
        "title": "Example",
        "description": "...",
        "image": "https://..."
      },
      "completeness_percent": 100.0
    },
    "structured_data": {
      "has_json_ld": true,
      "json_ld_count": 2,
      "types": ["Organization", "WebSite"]
    },
    "canonical": {
      "exists": true,
      "url": "https://example.com"
    },
    "issues": [
      "‚ùå 2 im√°genes sin atributo alt"
    ],
    "warnings": [
      "‚ö†Ô∏è  Title muy corto (14 chars). Recomendado: 50-60"
    ],
    "good_practices": [
      "‚úÖ Un √∫nico H1 correctamente definido",
      "‚úÖ Meta description con longitud √≥ptima",
      "‚úÖ Open Graph tags completos"
    ],
    "summary": {
      "total_issues": 1,
      "total_warnings": 1,
      "total_good_practices": 3
    }
  }
}
```

**Scoring:**
- `90-100`: Grade A (Excelente)
- `75-89`: Grade B (Bueno)
- `60-74`: Grade C (Aceptable)
- `45-59`: Grade D (Necesita mejoras)
- `0-44`: Grade F (Cr√≠tico)


---

## üìÅ Estructura del Proyecto

```
TP2/
‚îú‚îÄ‚îÄ server_scraping.py          # Servidor A (HTTP/Scraping)
‚îú‚îÄ‚îÄ server_processing.py        # Servidor B (TCP/Processing)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ common/                     # M√≥dulos compartidos
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ protocol.py            # Protocolo de comunicaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ rate_limiter.py        # ‚≠ê Rate limiting con Redis
‚îÇ   ‚îî‚îÄ‚îÄ cache.py               # ‚≠ê Sistema de cach√© con Redis
‚îÇ
‚îú‚îÄ‚îÄ scraper/                    # M√≥dulo de scraping
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ html_parser.py         # Parser HTML con BeautifulSoup
‚îÇ
‚îú‚îÄ‚îÄ processor/                  # M√≥dulo de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ screenshot.py          # Generador de screenshots
‚îÇ   ‚îú‚îÄ‚îÄ performance.py         # Analizador de rendimiento
‚îÇ   ‚îú‚îÄ‚îÄ image_processor.py     # Procesador de im√°genes
‚îÇ   ‚îú‚îÄ‚îÄ technology_detector.py # ‚≠ê Detector de tecnolog√≠as
‚îÇ   ‚îî‚îÄ‚îÄ seo_analyzer.py        # ‚≠ê Analizador de SEO
‚îÇ
‚îî‚îÄ‚îÄ tests/                      # Tests
    ‚îú‚îÄ‚îÄ test_protocol.py
    ‚îú‚îÄ‚îÄ test_server.py
    ‚îú‚îÄ‚îÄ test_rate_limiter.py   # ‚≠ê Tests de rate limiting
    ‚îî‚îÄ‚îÄ test_cache.py          # ‚≠ê Tests de cach√©
```

---

## üîß Configuraci√≥n Avanzada

### Rate Limiting Personalizado

```bash
# 5 requests por minuto
python server_scraping.py --max-requests 5

# Ventana de 30 segundos (ajustar en c√≥digo)
```

### Cach√© Personalizado

```bash
# TTL de 30 minutos (1800 segundos)
python server_scraping.py --cache-ttl 1800

# Deshabilitar cach√©
python server_scraping.py --no-cache
```

### Pool de Procesos

```bash
# 8 procesos en el pool
python server_processing.py -n 8

# Usar todos los CPUs disponibles (default)
python server_processing.py
```

---

## üìä Monitoreo

### Estad√≠sticas en tiempo real

```bash
# Watch de estad√≠sticas de cach√©
watch -n 1 'curl -s http://localhost:8000/cache/stats | python -m json.tool'

# Logs del servidor
tail -f server_scraping.log
tail -f server_processing.log
```

### Redis CLI

```bash
# Conectar a Redis
redis-cli

# Ver keys de rate limiting
KEYS rate:*

# Ver keys de cach√©
KEYS scraper:*

# Ver estad√≠sticas
INFO stats
```

---

## üêõ Troubleshooting

### Redis no conecta
```bash
# Verificar que Redis est√© corriendo
redis-cli ping
# Debe responder: PONG

# Iniciar Redis
redis-server
```


### Cach√© no funciona
```bash
# Verificar conexi√≥n a Redis
curl http://localhost:8000/health

# Limpiar cach√© corrupta
curl -X POST http://localhost:8000/cache/clear
```

---

## üìù Notas

- El rate limiting es por **dominio**, no por URL completa
- El cach√© diferencia entre scraping b√°sico (`full=false`) y completo (`full=true`)
- Los an√°lisis avanzados (tecnolog√≠as y SEO) **solo se ejecutan con `full=true`**
- Redis debe estar corriendo para rate limiting y cach√©
- El servidor B puede correr en m√°quina separada ajustando `--processing-host`

---

## üë• Autor

- **Estudiante**: Juan Cruz Rupcic
- **Materia**: Computaci√≥n II (Ingenier√≠a Inform√°tica)
- **Fecha**: Noviembre 2025
---
